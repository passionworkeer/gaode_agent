# è¿™æ˜¯æ–°ç‰ˆæœ¬çš„ run æ–¹æ³•ï¼Œéœ€è¦æ›¿æ¢åˆ° agent.py ä¸­

async def run(
    self, 
    user_input: str, 
    user_id: str = "default", 
    use_icl: bool = False,
    strategy: ExecutionStrategy = ExecutionStrategy.GRACEFUL_DEGRADE,
    stream_callback: Optional[Callable[[str], Coroutine]] = None
):
    """æ‰§è¡ŒAgentä¸»æµç¨‹ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼Œæ”¯æŒæµå¼è¾“å‡ºï¼‰
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        user_id: ç”¨æˆ·ID
        use_icl: æ˜¯å¦ä½¿ç”¨ICL
        strategy: æ‰§è¡Œç­–ç•¥ (å¤±è´¥å¤„ç†æ–¹å¼)
        stream_callback: åºŸå¼ƒå‚æ•°ï¼Œä¿ç•™ä¸ºäº†å‘åå…¼å®¹
    
    Yields:
        str: æµå¼è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
    """
    full_response = ""
    
    try:
        # âœ… ç¡®ä¿å·¥å…·å·²å¼‚æ­¥æ³¨å†Œï¼ˆåŒ…æ‹¬MCPï¼‰
        if not self.tools:
            await self._register_tools()
            logger.info(f"âœ… å·²æ³¨å†Œ {len(self.tools)} ä¸ªå·¥å…·")
        
        self.memory.update_history(user_id, {"role": "user", "content": user_input})
        memory = self.memory.load_memory(user_id)

        # 1. æ„å›¾åˆ†ç±»
        intent = await self._classify_intent(user_input, memory["conversation_history"])
        chunk = f"ğŸ” æ„å›¾åˆ†æå®Œæˆ: **{intent.value}**\n\n"
        full_response += chunk
        yield chunk

        # 2. æ ¹æ®æ„å›¾æ‰§è¡Œä¸åŒé€»è¾‘
        if intent == Intent.GENERAL_CHAT:
            # æ„å»ºç”¨äºæ™®é€šèŠå¤©çš„ Prompt
            memory = self.memory.load_memory(user_id)
            history = memory["conversation_history"]
            
            # ä½¿ç”¨ ICL Agent çš„ç¤ºä¾‹ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰ç¤ºä¾‹ï¼‰
            icl_examples = ""
            if use_icl and self.icl_agent.examples:
                icl_examples = "\n\n".join([f"ç¤ºä¾‹ {i+1}:\nç”¨æˆ·: {ex['query']}\nåŠ©æ‰‹: {ex['response']}" for i, ex in enumerate(self.icl_agent.examples)])
            
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ç”¨æˆ·è‡ªç„¶å¯¹è¯ã€‚
        {icl_examples}
        """
            # æ„å»º Messages
            messages = [{"role": "system", "content": system_prompt}]
            messages = MessageValidator.safe_extend_history(messages, history, max_count=5)
            messages.append({"role": "user", "content": user_input})
            validated_messages = MessageValidator.validate_messages(messages)

            # è°ƒç”¨æ¨¡å‹æµå¼ç”Ÿæˆ
            response_generator = self.model.stream_generate(validated_messages) 
            
            # æµå¼è¾“å‡º
            if hasattr(response_generator, '__aiter__'):
                async for chunk in response_generator:
                    full_response += chunk
                    yield chunk
            elif hasattr(response_generator, '__iter__'):
                for chunk in response_generator:
                    full_response += chunk
                    yield chunk
            else:
                # å¦‚æœä¸æ˜¯ç”Ÿæˆå™¨ï¼Œç›´æ¥è¾“å‡º
                chunk = str(response_generator)
                full_response += chunk
                yield chunk
                
            self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
            return

        if intent == Intent.TOOL_INFO_QUERY:
            reply = "æˆ‘å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š\n" + self.list_tools()
            full_response += reply
            yield reply
            self.memory.update_history(user_id, {"role": "assistant", "content": reply})
            return

        if intent == Intent.KNOWLEDGE_QUERY_ICL:
            chunk = "å¥½çš„ï¼Œæˆ‘å°†ä½¿ç”¨æˆ‘çš„çŸ¥è¯†åº“ä¸ºæ‚¨å¿«é€Ÿè§£ç­”...\n\n"
            full_response += chunk
            yield chunk
            tool = self.tools["in_context_learning_search"]
            result = await tool.arun(query=user_input)
            result_str = str(result)
            full_response += result_str
            yield result_str
            self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
            return

        if intent == Intent.KNOWLEDGE_QUERY_RAG:
            chunk = "æ­£åœ¨æŸ¥è¯¢æœ¬åœ°çŸ¥è¯†åº“...\n\n"
            full_response += chunk
            yield chunk
            tool = self.tools["rag_query"]
            result = await tool.arun(query=user_input)
            result_str = str(result)
            full_response += result_str
            yield result_str
            self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
            return

        # --- é»˜è®¤æ‰§è¡Œå¤æ‚ä»»åŠ¡é€»è¾‘ ---
        chunk = "å¥½çš„ï¼Œè¯·ç¨ç­‰ï¼Œæˆ‘æ­£åœ¨æ€è€ƒå¦‚ä½•å¤„ç†æ‚¨çš„è¯·æ±‚...\n\n"
        full_response += chunk
        yield chunk

        steps = await self.plan_tasks(user_input, user_id)
        if not steps:
            reply = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä¸ºæ‚¨çš„è¯·æ±‚è§„åˆ’å‡ºæœ‰æ•ˆçš„æ‰§è¡Œæ­¥éª¤ã€‚è¯·å°è¯•æ¢ä¸€ç§æ–¹å¼æé—®ï¼Œæˆ–è€…æè¿°å¾—æ›´å…·ä½“ä¸€äº›ã€‚"
            full_response += reply
            yield reply
            self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
            return

        chunk = "æˆ‘å·²ç»åˆ¶å®šäº†å¦‚ä¸‹è®¡åˆ’ï¼š\n"
        full_response += chunk
        yield chunk
        for i, step in enumerate(steps):
            chunk = f"   - æ­¥éª¤ {i+1}: {step.goal}\n"
            full_response += chunk
            yield chunk
        chunk = "\nç°åœ¨ï¼Œæˆ‘å°†å¼€å§‹æ‰§è¡Œè¿™äº›æ­¥éª¤...\n\n"
        full_response += chunk
        yield chunk

        step_context = StepContext()
        steps_results = []
        
        for i, step in enumerate(steps):
            chunk = f"**æ­£åœ¨æ‰§è¡Œæ­¥éª¤ {i+1}: {step.goal}**\n"
            full_response += chunk
            yield chunk
            try:
                resolved_params = step_context.replace_placeholders(step.parameters, i)
                resolved_step = TaskStep(goal=step.goal, tool_name=step.tool_name, parameters=resolved_params)
                logger.info(f"æ­¥éª¤ {i}: {step.goal} | è§£æåå‚æ•°: {resolved_params}")
                chunk = f"   - è°ƒç”¨å·¥å…·: `{step.tool_name}`\n"
                full_response += chunk
                yield chunk
                chunk = f"   - æä¾›å‚æ•°: `{json.dumps(resolved_params, ensure_ascii=False, indent=2)}`\n"
                full_response += chunk
                yield chunk
            except ValueError as e:
                error_msg = f"å‚æ•°è§£æå¤±è´¥: {e}"
                logger.error(f"æ­¥éª¤ {i} {error_msg}")
                chunk = f"   - âŒ **é”™è¯¯**: {error_msg}\n"
                full_response += chunk
                yield chunk
                if strategy == ExecutionStrategy.FAIL_FAST:
                    final_reply = f"æŠ±æ­‰ï¼Œä»»åŠ¡åœ¨'{step.goal}'æ­¥éª¤ä¸­æ–­ï¼Œå› ä¸ºå‚æ•°å‡†å¤‡å¤±è´¥ã€‚"
                    full_response += f"\n{final_reply}"
                    yield f"\n{final_reply}"
                    self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
                    return
                success, result = False, {"error": error_msg}
                resolved_step = step
            else:
                success, result = await self.execute_step(resolved_step, user_id)
            
            if not success:
                error_msg = f"æ­¥éª¤ {i+1} æ‰§è¡Œå¤±è´¥: {result}"
                logger.error(error_msg)
                chunk = f"   - âŒ **é”™è¯¯**: {result}\n\n"
                full_response += chunk
                yield chunk
                if strategy == ExecutionStrategy.FAIL_FAST:
                    reply = f"æŠ±æ­‰ï¼Œåœ¨æ‰§è¡Œ'{step.goal}'æ—¶é‡åˆ°é—®é¢˜ï¼š{result}\n\nè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"
                    full_response += reply
                    yield reply
                    self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
                    return
                elif strategy == ExecutionStrategy.GRACEFUL_DEGRADE:
                    logger.info(f"æ­¥éª¤å¤±è´¥ï¼Œé™çº§ä¸ºçº¯å¯¹è¯æ¨¡å¼")
                    chunk = "å“å‘€ï¼Œæ‰§è¡Œè®¡åˆ’é‡åˆ°äº†ä¸€ç‚¹å°é—®é¢˜ã€‚æˆ‘å°†å°è¯•æ ¹æ®ç°æœ‰ä¿¡æ¯ä¸ºæ‚¨æ€»ç»“å›ç­”ã€‚\n"
                    full_response += chunk
                    yield chunk
                    break
            
            try:
                if isinstance(result, str):
                    result_dict = RobustJSONParser.parse(result)
                    step_context.set_result(i, result_dict if result_dict and isinstance(result_dict, dict) else {"raw": result, "success": success})
                else:
                    step_context.set_result(i, result if isinstance(result, dict) else {"raw": str(result), "success": success})
                logger.info(f"âœ… æ­¥éª¤ {i} ç»“æœå·²ä¿å­˜")
                chunk = f"   - âœ… **æˆåŠŸ**: æ­¥éª¤å®Œæˆï¼Œç»“æœå·²ä¿å­˜ã€‚\n\n"
                full_response += chunk
                yield chunk
            except Exception as parse_error:
                logger.warning(f"âš ï¸ æ­¥éª¤ {i} ç»“æœä¿å­˜å¤±è´¥: {parse_error}, ä¿å­˜åŸå§‹å€¼")
                step_context.set_result(i, {"raw": str(result), "success": success, "error": str(parse_error)})
                chunk = f"   - âš ï¸ **è­¦å‘Š**: æ­¥éª¤ç»“æœä¿å­˜æ—¶é‡åˆ°é—®é¢˜: {parse_error}\n\n"
                full_response += chunk
                yield chunk

            steps_results.append((resolved_step, result))
            self.memory.update_history(user_id, {"role": "system", "content": f"æ‰§è¡Œæ­¥éª¤ {i}: {step.goal}\nå·¥å…·: {step.tool_name}\nç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}"})

        chunk = "æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæ¯•ï¼Œç°åœ¨æˆ‘å°†ä¸ºæ‚¨æ•´åˆæœ€ç»ˆç»“æœ...\n\n---\n\n"
        full_response += chunk
        yield chunk
        
        # è°ƒç”¨ç»“æœæ•´åˆæ–¹æ³•ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰
        async for chunk in self.integrate_results_stream(user_input, steps_results, user_id):
            full_response += chunk
            yield chunk

        self.memory.update_history(user_id, {"role": "assistant", "content": full_response})

    except Exception as e:
        logger.error(f"Agent ä¸»æµç¨‹å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        error_message = f"\n\n--- \n**ç³»ç»Ÿé”™è¯¯** \næŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†ä¸€ä¸ªæ„å¤–çš„é—®é¢˜: `{str(e)}` \nè¯·ç¨åå†è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
        full_response += error_message
        yield error_message
        # ç¡®ä¿å³ä½¿åœ¨é¡¶å±‚å¼‚å¸¸ä¸­ï¼Œæœ€ç»ˆçš„é”™è¯¯ä¿¡æ¯ä¹Ÿè¢«è®°å½•
        self.memory.update_history(user_id, {"role": "assistant", "content": full_response})
